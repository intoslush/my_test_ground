{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0620 09:33:24.509010 56 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:24.512395 56 compiler.py:956] Jittor(1.3.8.5) src: /root/miniconda3/envs/pyt/lib/python3.10/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:24.514954 56 compiler.py:957] g++ at /usr/bin/g++(9.4.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:24.515615 56 compiler.py:958] cache_path: /root/.cache/jittor/jt1.3.8/g++9.4.0/py3.10.11/Linux-5.4.0-16xfa/IntelRXeonRPlax40/default\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:24.518702 56 __init__.py:411] Found /usr/local/cuda/bin/nvcc(11.7.99) at /usr/local/cuda/bin/nvcc.\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:24.521289 56 __init__.py:411] Found addr2line(2.34) at /usr/bin/addr2line.\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:24.617382 56 compiler.py:1011] cuda key:cu11.7.99_sm_75\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0620 09:33:24.964167 56 __init__.py:227] Total mem: 30.60GB, using 10 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:25.147764 56 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:25.363856 56 init.cc:62] Found cuda archs: [75,]\u001b[m\n",
      "\u001b[38;5;2m[i 0620 09:33:28.335259 56 cuda_flags.cc:49] CUDA enabled.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包导入成功\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "from PIL import Image\n",
    "import jclip as clip\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import random\n",
    "# from colorama import Fore, Back, Style, init\n",
    "# init()\n",
    "\n",
    "jt.flags.use_cuda = 1\n",
    "print(\"包导入成功\")\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--split', type=str, default='A')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "model, preprocess = clip.load(\"../data/ViT-B-32.pkl\")\n",
    "imgs_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode这块后面可以强化下\n",
    "def encode_pre_word(c)->str:\n",
    "    \"\"\"更具输入的名字返回一句话\"\"\"\n",
    "    seq = 'a photo of ' + c\n",
    "    return seq\n",
    "\n",
    "def ret_class_name_dic()->dict:\n",
    "    \"\"\"返回数字映射到动物名字的字典\"\"\"\n",
    "    classes = open('../data/classname.txt').read().splitlines()#这是一个包含所有类的列表\n",
    "    class_name_dic={}#这是数字映射到动物名字的字典\n",
    "    for i in classes:\n",
    "        name,idx = i.split(' ')\n",
    "        if idx not in class_name_dic:\n",
    "            class_name_dic[idx]=name\n",
    "    return class_name_dic\n",
    "\n",
    "# class_name_dic=ret_class_name_dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练的数据处理\n",
    "def train_data()->list:\n",
    "    \"\"\"返回元素为玩意的列表['TrainSet/Animal/Bee/57.jpg', '1'] 每类四个的打乱列表\"\"\"\n",
    "    train_labels = open('../data/train.txt').read().splitlines()\n",
    "    train_data_dic={}#每类的图片的字典\n",
    "    for i in train_labels:\n",
    "        path,class_name=i.split(' ')\n",
    "        if class_name in train_data_dic:\n",
    "            train_data_dic[class_name].append([path,class_name])\n",
    "        else:\n",
    "            train_data_dic[class_name]=[]\n",
    "    # 训练集的要训练的每类四张的列表\n",
    "    ret_list=[]#用于返回每类四张的列表\n",
    "    for i in train_data_dic:\n",
    "        ret_list+=random.sample(train_data_dic[i],4)\n",
    "    return random.shuffle(ret_list)# 内存不够再优化\n",
    "\n",
    "# train_data=train_data()\n",
    "#测试数据列表返回\n",
    "def test_data(train_data:list)->list:\n",
    "    \"\"\"返回元素为玩意的列表['TrainSet/Animal/Bee/57.jpg 1] 共计3000个用于测试\n",
    "    同时剔除训练集中的元素\"\"\"\n",
    "    set1=set(train_data)\n",
    "    train_labels = open('../data/train.txt').read().splitlines()\n",
    "    result = [item for item in train_labels if item not in set1] \n",
    "    return random.sample(result,3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把句子和图片进行encoding\n",
    "train_img_features = []\n",
    "train_word_features=[]\n",
    "count=0\n",
    "with jt.no_grad():\n",
    "    class_name_dic=ret_class_name_dic()\n",
    "    for info in tqdm(train_data):\n",
    "\n",
    "        img,indx=info\n",
    "        img = os.path.join(imgs_dir, img)\n",
    "        image = Image.open(img)\n",
    "        image = preprocess(image).unsqueeze(0)\n",
    "        image_features = model.encode_image(image)\n",
    "        # print(\"能成功运行?\",\"image_features的shape是\",image_features.shape)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        train_img_features.append(image_features)\n",
    "\n",
    "        a_seq=encode_pre_word(class_name_dic[indx])#转为句子\n",
    "        token=clip.tokenize(a_seq)\n",
    "        \n",
    "        text_features=model.encode_text(token)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        print(\"text_features的shape为\",text_features.shape)\n",
    "        train_word_features.append(text_features)\n",
    "        count+=1\n",
    "        if count==5 :\n",
    "            break\n",
    "\n",
    "train_features = jt.cat(train_img_features).numpy()#(1496, 512)\n",
    "train_labels = jt.cat(train_word_features).numpy()#(1496,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把原模型的最后一层给改了,然后最后两层微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "    visual: VisionTransformer(\n",
      "        conv1: Conv(3, 768, (32, 32), (32, 32), (0, 0), (1, 1), 1, None, None, Kw=None, fan=None, i=None, bound=None)\n",
      "        ln_pre: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "        transformer: Transformer(\n",
      "            resblocks: Sequential(\n",
      "                0: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                1: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                2: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                3: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                4: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                5: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                6: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                7: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                8: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                9: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                10: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                11: ResidualAttentionBlock(\n",
      "                    attn: MultiheadAttention(\n",
      "                        out_proj: Linear(768, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_1: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                    mlp: MLP(\n",
      "                        c_fc: Linear(768, 3072, float32[3072,], None)\n",
      "                        gelu: QuickGELU(None, None)\n",
      "                        c_proj: Linear(3072, 768, float32[768,], None)\n",
      "                    )\n",
      "                    ln_2: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "        ln_post: LayerNorm((768,), 1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    transformer: Transformer(\n",
      "        resblocks: Sequential(\n",
      "            0: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            1: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            2: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            3: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            4: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            5: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            6: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            7: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            8: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            9: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            10: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            11: ResidualAttentionBlock(\n",
      "                attn: MultiheadAttention(\n",
      "                    out_proj: Linear(512, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_1: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "                mlp: MLP(\n",
      "                    c_fc: Linear(512, 2048, float32[2048,], None)\n",
      "                    gelu: QuickGELU(None, None)\n",
      "                    c_proj: Linear(2048, 512, float32[512,], None)\n",
      "                )\n",
      "                ln_2: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    token_embedding: Embedding(49408, 512, None, dtype=None)\n",
      "    ln_final: LayerNorm((512,), 1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((512,), 1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ln_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评分函数 s(image, text)\n",
    "def score_function(image_embedding, text_embedding):\n",
    "    # 使用余弦相似度作为评分函数\n",
    "    return jt.matmul(image_embedding, text_embedding.transpose(1, 0))\n",
    "\n",
    "# 定义 CLIP 损失函数\n",
    "def clip_loss(image_embeddings, text_embeddings, temperature=1.0):\n",
    "    batch_size = image_embeddings.shape[0]\n",
    "    \n",
    "    # 计算所有文本描述与图像之间的评分\n",
    "    scores = score_function(image_embeddings, text_embeddings)\n",
    "    \n",
    "    # 计算对比损失函数\n",
    "    logits = scores / temperature\n",
    "    logits_max, _ = jt.max(logits, dim=1, keepdims=True)\n",
    "    logits = logits - logits_max.detach()  # 避免数值不稳定性\n",
    "    exp_logits = jt.exp(logits)\n",
    "    softmax_probs = exp_logits / jt.sum(exp_logits, axis=1, keepdims=True)\n",
    "    # 对角线位置的 softmax 概率即为对应的文本描述与图像匹配的概率\n",
    "    correct_probs = jt.diag(softmax_probs)\n",
    "    # 计算对比损失\n",
    "    loss = -jt.log(correct_probs + 1e-6).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
