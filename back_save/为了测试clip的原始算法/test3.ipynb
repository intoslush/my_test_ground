{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from balanced_batch_sampler import BalancedBatchSampler\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "# from torch.utils.data import Dataset\n",
    "import gc\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH=30\n",
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TO ADD :\n",
    "# Gradient Checkpointing\n",
    "# Filter out bias from weight decay\n",
    "# Decaying learning rate with cosine schedule\n",
    "# Half-precision Adam statistics\n",
    "# Half-precision stochastically rounded text encoder weights were used\n",
    "\n",
    "#BATCH_SIZE must larger than 1\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path,list_txt,nidx):\n",
    "\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = clip.tokenize(list_txt) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
    "        # self.title=self.title\n",
    "        self.nidx=nidx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        ndix=self.nidx[idx]\n",
    "        return image,title,ndix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_class_name_dic()->dict:\n",
    "    \"\"\"返回动物名字到数字和数字映射到动物名的字典\"\"\"\n",
    "    classes = open('data/classname.txt').read().splitlines()#这是一个包含所有类的列表\n",
    "    class_name_dic_num={}\n",
    "    class_name_dic_name={}\n",
    "    for i in classes:\n",
    "        name,idx = i.split(' ')\n",
    "        c = name\n",
    "        if c.startswith('Animal'):\n",
    "            c = c[7:]\n",
    "        if c.startswith('Thu-dog'):\n",
    "            c = c[8:]\n",
    "        if c.startswith('Caltech-101'):\n",
    "            c = c[12:]\n",
    "        if c.startswith('Food-101'):\n",
    "            c = c[9:]\n",
    "        if c not in class_name_dic_name:\n",
    "            class_name_dic_name[c]=idx\n",
    "            class_name_dic_num[idx]=c\n",
    "        else:\n",
    "            print(name,\"already exist!!\")\n",
    "    return class_name_dic_name,class_name_dic_num\n",
    "class_name_dic_name,class_name_dic_num=ret_class_name_dic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_pic_patch()->dict:\n",
    "    \"\"\"返回每类四张,的路径和标签\"\"\"\n",
    "    r_path=[]\n",
    "    r_class_num=[]\n",
    "    info = open('data/train.txt').read().splitlines()\n",
    "    \n",
    "    class_check=0\n",
    "    temp_path=[]\n",
    "    temp_class=[]\n",
    "    for i in info:\n",
    "        path,class_num=i.split(' ')\n",
    "        path=\"data/\"+path\n",
    "        if class_check==int(class_num):\n",
    "            temp_path.append(path)\n",
    "            temp_class.append(class_num)\n",
    "        else:\n",
    "            class_check=int(class_num)\n",
    "\n",
    "            r_path+=random.sample(temp_path,4)\n",
    "            r_class_num+=random.sample(temp_class,4)\n",
    "            temp_path=[path]\n",
    "            temp_class=[class_num]\n",
    "    r_path+=random.sample(temp_path,4)\n",
    "    r_class_num+=random.sample(temp_class,4)\n",
    "    return r_path,r_class_num\n",
    "\n",
    "path,class_num=ret_pic_patch()\n",
    "print(path[0:5],\"\\n\",class_num[0:5])\n",
    "print(class_name_dic_num['0'])\n",
    "list_text_raw=[\"a photo of a \"+class_name_dic_num[i] for i in class_num]\n",
    "print(list_text_raw[0:5])\n",
    "print(len(list_text_raw),len(class_name_dic_name.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use your own data\n",
    "list_image_path = path#['folder/image1.jpg','folder2/image2.jpg'] \n",
    "list_txt = list_text_raw\n",
    "\n",
    "dataset = image_title_dataset(list_image_path,list_txt,class_num)\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE,shuffle=True) #Define your own dataloader\n",
    "\n",
    "# #https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "else :\n",
    "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = open('data/classname.txt').read().splitlines()\n",
    "new_classes = []\n",
    "for c in classes:\n",
    "    c = c.split(' ')[0]\n",
    "    if c.startswith('Animal'):\n",
    "        c = c[7:]\n",
    "    if c.startswith('Thu-dog'):\n",
    "        c = c[8:]\n",
    "    if c.startswith('Caltech-101'):\n",
    "        c = c[12:]\n",
    "    if c.startswith('Food-101'):\n",
    "        c = c[9:]\n",
    "    c = 'a photo of ' + c\n",
    "    new_classes.append(c)\n",
    "\n",
    "print(new_classes[0:5])\n",
    "text2 = clip.tokenize(new_classes).to(device)\n",
    "# text2 = model.encode_text(text2).to(device)\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add your own code to track the training progress.\n",
    "for epoch in range(EPOCH):\n",
    "    total_count=0\n",
    "    total_count1=0\n",
    "    total_count5=0\n",
    "    count_loss=0\n",
    "    model.train()\n",
    "    for batch in train_dataloader :\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts,idx = batch \n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        # print(\"images.shape\",images.shape)\n",
    "        # images= model.encode_image(images).to(device)\n",
    "        # print(\"images.shape\",images.shape)\n",
    "        # texts = model.encode_text(texts).to(device)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        # print(\"total_loss是:\",total_loss)\n",
    "        count_loss+=total_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    for batch in train_dataloader :\n",
    "        images,texts,idx = batch \n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        \n",
    "        # image_features = model.encode_image(images)\n",
    "        # image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits_per_image, logits_per_text =model(images, text2)\n",
    "        # print(\"这样可以?\",logits_per_image.shape)\n",
    "        \n",
    "        text_probs=logits_per_image.softmax(dim=-1)\n",
    "        # text_probs = (100.0 *image_features @ text_features.transpose(0, 1)).softmax(dim=-1)\n",
    "        # top5 predictions\n",
    "        # _, top_labels = text_probs[0].topk(5)\n",
    "        # print(top_labels.shape,top_labels[0])\n",
    "        for i in range(len(idx)):\n",
    "            top5=text_probs[i].topk(5).indices.tolist()\n",
    "            # print(top5)\n",
    "            # print(int(idx[i]))\n",
    "            if int(idx[i]) in top5:\n",
    "                total_count5+=1\n",
    "                if int(idx[i])==top5[0]:\n",
    "                    total_count1+=1\n",
    "        total_count+=BATCH_SIZE\n",
    "        # print(\"64个里面top1有\",top1_count,\"\\ntop5有\",top5_count)\n",
    "        \n",
    "    print(\"训练集准确率top1\",total_count1/total_count,\"\\ntop5有\",total_count5/total_count)\n",
    "    print('loss为',count_loss)\n",
    "    del count_loss,total_count1,total_count,total_count5\n",
    "    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': total_loss,\n",
    "#         }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename\n",
    "\n",
    "# # 加载之前训练的模型\n",
    "# model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "# checkpoint = torch.load(\"model_checkpoint/model_10.pt\")\n",
    "\n",
    "# # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
